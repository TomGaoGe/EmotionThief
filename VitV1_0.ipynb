{"cells":[{"cell_type":"code","metadata":{"cell_id":"fc88b4887c954fceb10a6676910c27ac","deepnote_cell_type":"code"},"source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt\nfrom einops.layers.torch import Rearrange\n\n# Define transformations and load datasets\ntransform = transforms.Compose([\n    transforms.Grayscale(),  # ViT generally uses RGB, might consider using color images\n    transforms.Resize((224, 224)),  # Adjust size for ViT, typically 224x224\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])  # Adjust mean and std if using RGB\n])\n\ntrain_dataset = datasets.ImageFolder(root='archive/train', transform=transform)\ntest_dataset = datasets.ImageFolder(root='archive/test', transform=transform)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n\n# Check for device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Define a simple Vision Transformer Model\nclass PatchEmbedding(nn.Module):\n    def __init__(self, patch_size=16, in_channels=1, embed_size=1024):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(in_channels, embed_size, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)  # B, E, H/P, W/P\n        x = x.flatten(2)  # B, E, N (N is number of patches)\n        x = x.transpose(1, 2)  # B, N, E\n        return x\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size=224, patch_size=16, num_classes=7, dim=1024, depth=6, heads=8, mlp_dim=2048):\n        super(VisionTransformer, self).__init__()\n        num_patches = (image_size // patch_size) ** 2\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.patch_embedding = PatchEmbedding(patch_size, 1, dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(dim, heads, mlp_dim, dropout=0.1),\n            num_layers=depth\n        )\n        self.to_cls_token = nn.Identity()\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, img):\n        p = self.patch_embedding(img)\n        b, n, _ = p.shape\n        cls_tokens = self.cls_token.expand(b, -1, -1)\n        x = torch.cat((cls_tokens, p), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.transformer(x)\n        x = self.to_cls_token(x[:, 0])\n        return self.mlp_head(x)\n\n# Initialize model, optimizer, loss function, and scheduler\nmodel = VisionTransformer().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\nscheduler = ExponentialLR(optimizer, gamma=0.95)\n\n# Training and validation loop, plotting omitted for brevity\n# Refer to the previously provided code for the complete training loop and plotting\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Assuming the model, optimizer, criterion, and scheduler are already defined and initialized\n\n# Lists to store metrics\ntrain_losses = []\nval_losses = []\nlearning_rates = []\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item() * images.size(0)\n    \n    # Calculate average losses\n    avg_train_loss = total_train_loss / len(train_loader.dataset)\n    train_losses.append(avg_train_loss)\n    learning_rates.append(scheduler.get_last_lr()[0])\n\n    # Validation phase\n    model.eval()\n    total_val_loss = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            total_val_loss += loss.item() * images.size(0)\n    avg_val_loss = total_val_loss / len(test_loader.dataset)\n    val_losses.append(avg_val_loss)\n\n    # Update learning rate\n    scheduler.step()\n\n    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}, Learning Rate: {scheduler.get_last_lr()[0]}')","block_group":"fc88b4887c954fceb10a6676910c27ac","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0a047d2a-49ae-49e7-9442-011956428446' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"a56ef9caef614d61bf64e08acf82509f","deepnote_execution_queue":[]}}